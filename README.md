# Aiding Eyes - AI-Powered Assistive Mobility Tool for Visually Impaired

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.12](https://img.shields.io/badge/Python-3.12-blue.svg)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.115.0-brightgreen.svg)](https://fastapi.tiangolo.com/)

**Aiding Eyes** is an AI-powered web application that processes uploaded videos to detect obstacles, traffic signs, vehicles, and pedestrians using YOLOv8 object detection. It generates real-time audio narration guidance for visually impaired users to navigate safely.

## âœ¨ Features

- ğŸ¥ **Video Upload & Recording** - Drag & drop or webcam recording
- ğŸ§  **Dual YOLOv8 Detection** - Official + custom-trained models
- ğŸ”Š **Text-to-Speech Narration** - Real-time audio guidance
- ğŸ¨ **Visual Overlays** - Bounding boxes on detected objects
- ğŸ›¡ï¸ **Secure Authentication** - Clerk JWT token verification
- ğŸ“± **Responsive UI** - React + TypeScript + shadcn/ui
- ğŸš€ **FastAPI Backend** - Production-ready API server

## ğŸ—ï¸ Tech Stack

| Frontend | Backend | AI/ML | Infrastructure |
|----------|---------|-------|----------------|
| React 18 | FastAPI | YOLOv8 | Supabase |
| TypeScript | Python 3.12 | PyTorch | Clerk Auth |
| Vite | Uvicorn | Transformers | SQLite |
| shadcn/ui | OpenCV | MoviePy | Docker-ready |

## ğŸš€ Quick Start

### Prerequisites
- Python 3.12+
- Node.js 18+
- Git

### 1. Clone & Setup

```bash
git clone <your-repo-url>
cd aiding-eyes

2. Backend Setup
cd backend
pip install -r requirements.txt
# Copy your Supabase/Clerk keys to .env
cp .env.example .env
uvicorn main:app --reload --host 0.0.0.0 --port 5000

3. Frontend Setup
cd frontend
npm install
npm run dev

4. Access Application
Backend: http://localhost:5000
Frontend: http://localhost:8080

ğŸ“ Project Structure
aiding-eyes/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py                 # FastAPI server
â”‚   â”œâ”€â”€ final_implementation_2.py # YOLO + TTS processing
â”‚   â”œâ”€â”€ requirements.txt        # Python dependencies
â”‚   â”œâ”€â”€ uploads/               # Temporary uploads
â”‚   â””â”€â”€ processed/             # Output videos
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â””â”€â”€ VideoUpload.tsx    # Main upload component
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ vite.config.ts
â””â”€â”€ README.md


ğŸ”§ Environment Variables
backend/.env
SUPABASE_URL=your_supabase_url
SUPABASE_KEY=your_supabase_key

frontend/.env
VITE_CLERK_PUBLISHABLE_KEY=pk_test_...
VITE_BACKEND_URL=http://localhost:5000


ğŸ§ª Model Files Required
Place these files in backend/:
yolov8n.pt          # Official YOLOv8 nano (auto-downloads)
best.pt            # Your custom-trained model

ğŸ¯ Usage
Upload video via drag & drop or webcam recording
Process - AI detects obstacles & generates narration
Download processed video with bounding boxes + audio guidance
Real-time feedback for safe navigation

ğŸ› ï¸ Development Commands
Backend
cd backend
pip install -r requirements.txt
uvicorn main:app --reload --host 0.0.0.0 --port 5000

Frontend
cd frontend
npm install
npm run dev

ğŸ“ˆ Performance
Frame Skip: Adjustable (default: 5)
Memory Usage: ~2-4GB per video
Processing Time: 10-30s for 30s video
Supported Formats: MP4, AVI, MOV, WMV, MKV

Built with â¤ï¸ for accessibility